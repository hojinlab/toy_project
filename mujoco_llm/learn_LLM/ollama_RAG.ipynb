{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "683cbdd9",
   "metadata": {},
   "source": [
    "Qwen3 (0.6B) ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬, [ì‹¤ìŠµ 1: LLMì„ í•¨ìˆ˜ì²˜ëŸ¼ ì‚¬ìš©í•˜ê¸°]ì™€ [ì‹¤ìŠµ 2: ë¬¸ì„œ ê¸°ë°˜ ì „ë¬¸ê°€(On-Device RAG)] ì§„í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e7aa395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76162b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qwen3ëŠ” ì¶”ë¡  ëŠ¥ë ¥ì´ ê°•í™”ë˜ì–´ ì‘ì€ ì‚¬ì´ì¦ˆë¡œë„ ë˜‘ë˜‘í•œ ëŒ€ë‹µì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
    "llm = ChatOllama(model=\"qwen3:0.6b\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c10086",
   "metadata": {},
   "source": [
    "[ì‹¤ìŠµ 1] LLMì„ í•¨ìˆ˜ì²˜ëŸ¼ ì‚¬ìš©í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f743f57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-shot í”„ë¡¬í”„íŠ¸ (ì˜ˆì‹œë¥¼ ì£¼ì…)\n",
    "# ëª¨ë¸ì—ê²Œ \"ì—¬ëŸ¬ ê°œê°€ ë‚˜ì˜¤ë©´ ë¦¬ìŠ¤íŠ¸([])ë¡œ ë¬¶ì–´ë¼\"ëŠ” ê²ƒì„ ì˜ˆì‹œë¡œ í•™ìŠµì‹œí‚µë‹ˆë‹¤.\n",
    "template = \"\"\"\n",
    "You are a precise Data Extraction Module.\n",
    "Extract hardware status from the input text into a JSON List format.\n",
    "\n",
    "Examples:\n",
    "Input: \"Battery is 12V and Camera is OK.\"\n",
    "Output: [\n",
    "    {{\"component\": \"Battery\", \"status\": \"Normal\", \"value\": 12}},\n",
    "    {{\"component\": \"Camera\", \"status\": \"OK\", \"value\": null}}\n",
    "]\n",
    "\n",
    "Input: {input}\n",
    "Output:\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72b3403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\"component\": \"LiDAR\", \"status\": \"Normal\", \"value\": null},\n",
      "    {\"component\": \"Motor\", \"status\": \"Warning\", \"value\": 45}\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# ì˜ˆ: \"ì§€ê¸ˆ ë¼ì´ë‹¤ ì„¼ì„œëŠ” ì •ìƒ, ëª¨í„° ì˜¨ë„ ê²½ê³  ìƒíƒœ\"\n",
    "user_input = \"LiDAR ì„¼ì„œ-> ì •ìƒ ì‘ë™ ì¤‘, ëª¨í„° ì˜¨ë„ê°€ 45ë„ë¼ì„œ ê²½ê³  ìƒíƒœì•¼.\"\n",
    "\n",
    "response = chain.invoke({\"input\": user_input})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d63ec3",
   "metadata": {},
   "source": [
    "**[ì‹¤ìŠµ 2]: ë¬¸ì„œ ê¸°ë°˜ ì „ë¬¸ê°€(On-Device RAG)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db4a7055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from pypdf import PdfReader\n",
    "\n",
    "# ì„¤ì • ë° API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”(ì„ë² ë”© ë‹´ë‹¹)\n",
    "load_dotenv()\n",
    "api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    "client = genai.Client(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1820f7d",
   "metadata": {},
   "source": [
    "ìœ í‹¸ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c299ade9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„ë² ë”© ë° ê²€ìƒ‰ í•¨ìˆ˜ ì •ì˜ (Geminiì˜ 'ëˆˆ' ì—­í• )\n",
    "def get_embedding(text, task_type=\"RETRIEVAL_DOCUMENT\"):\n",
    "    response = client.models.embed_content(\n",
    "        model=\"text-embedding-004\",\n",
    "        contents=text,\n",
    "        config=types.EmbedContentConfig(task_type=task_type)\n",
    "    )\n",
    "    return response.embeddings[0].values\n",
    "\n",
    "def search_pdf(query, chunks, chunk_embeddings):\n",
    "    # ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜\n",
    "    query_vec = get_embedding(query, task_type=\"RETRIEVAL_QUERY\")\n",
    "    \n",
    "    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "    scores = [np.dot(query_vec, doc_vec) for doc_vec in chunk_embeddings]\n",
    "    best_idx = np.argmax(scores)\n",
    "    \n",
    "    return chunks[best_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e16233",
   "metadata": {},
   "source": [
    "ë©”ì¸ ì‹¤í–‰ ë¡œì§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25ab1130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF 'src/2025_vla.pdf' ë¡œë”© ë° ì²˜ë¦¬ ì¤‘...\n",
      "Vectorizing 3 pages... (Gemini Embedding)\n",
      "ë²¡í„°í™” ì™„ë£Œ!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PDF ì½ê¸° ë° ë²¡í„°í™” (ë°ì´í„° ì¤€ë¹„)\n",
    "pdf_path = \"src/2025_vla.pdf\" \n",
    "print(f\"PDF '{pdf_path}' ë¡œë”© ë° ì²˜ë¦¬ ì¤‘...\")\n",
    "\n",
    "reader = PdfReader(pdf_path)\n",
    "chunks = [page.extract_text() for page in reader.pages if page.extract_text()]\n",
    "\n",
    "# Geminië¡œ ë¬¸ì„œ ì „ì²´ ì„ë² ë”© (ìµœì´ˆ 1íšŒ ì‹¤í–‰)\n",
    "print(f\"Vectorizing {len(chunks)} pages... (Gemini Embedding)\")\n",
    "chunk_embeddings = [get_embedding(chunk) for chunk in chunks]\n",
    "print(\"ë²¡í„°í™” ì™„ë£Œ!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b2d53a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¹„êµí•  ì§ˆë¬¸\n",
    "test_question = \"ì´ë²ˆ VLA íŠ¹ê°•ì˜ ì‹¤ìŠµ ì¡°êµ(TA) ì´ë¦„ê³¼ ë¬¸ì˜ ì´ë©”ì¼ ì£¼ì†Œë¥¼ ì•Œë ¤ì¤˜.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ae138c",
   "metadata": {},
   "source": [
    "[Case 1] âŒ RAG ì ìš© ì „"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25e53f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [âŒ RAG ì ìš© ì „ ì§ˆë¬¸: ì´ë²ˆ VLA íŠ¹ê°•ì˜ ì‹¤ìŠµ ì¡°êµ(TA) ì´ë¦„ê³¼ ë¬¸ì˜ ì´ë©”ì¼ ì£¼ì†Œë¥¼ ì•Œë ¤ì¤˜.] ---\n",
      "ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”.  \n",
      "ì§ˆë¬¸: ì´ë²ˆ VLA íŠ¹ê°•ì˜ ì‹¤ìŠµ ì¡°êµ(TA) ì´ë¦„ê³¼ ë¬¸ì˜ ì´ë©”ì¼ ì£¼ì†Œë¥¼ ì•Œë ¤ì¤˜.  \n",
      "\n",
      "**ë‹µë³€:**  \n",
      "ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”.  \n",
      "ì§ˆë¬¸: ì´ë²ˆ VLA íŠ¹ê°•ì˜ ì‹¤ìŠµ ì¡°êµ(TA) ì´ë¦„ê³¼ ë¬¸ì˜ ì´ë©”ì¼ ì£¼ì†Œë¥¼ ì•Œë ¤ì¤˜.  \n",
      "\n",
      "**ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”:**  \n",
      "- **ì‹¤ìŠµ ì¡°êµ(TA) ì´ë¦„:** [ì‹¤ìŠµ ì¡°êµ ì´ë¦„]  \n",
      "- **ë¬¸ì˜ ì´ë©”ì¼ ì£¼ì†Œ:** [ë¬¸ì˜ ì´ë©”ì¼ ì£¼ì†Œ]  \n",
      "\n",
      "ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”.\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"--- [âŒ RAG ì ìš© ì „ ì§ˆë¬¸: {test_question}] ---\")\n",
    "\n",
    "template_no_rag = \"\"\"\n",
    "ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”.\n",
    "ì§ˆë¬¸: {question}\n",
    "\"\"\"\n",
    "prompt_no_rag = ChatPromptTemplate.from_template(template_no_rag)\n",
    "\n",
    "# Chain ì‹¤í–‰\n",
    "chain_no_rag = prompt_no_rag | llm | StrOutputParser()\n",
    "response_before = chain_no_rag.invoke({\"question\": test_question})\n",
    "\n",
    "print(response_before)\n",
    "print(\"-------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01d490c",
   "metadata": {},
   "source": [
    "[Case 2] âœ… RAG ì ìš© í›„ (ê²€ìƒ‰ + ë‹µë³€)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68f018b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [RAG ì ìš© í›„ ì§ˆë¬¸: ì´ë²ˆ VLA íŠ¹ê°•ì˜ ì‹¤ìŠµ ì¡°êµ(TA) ì´ë¦„ê³¼ ë¬¸ì˜ ì´ë©”ì¼ ì£¼ì†Œë¥¼ ì•Œë ¤ì¤˜.] ---\n",
      "ğŸ” [ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš© ì¼ë¶€]:\n",
      "2025 ë™ê³„  ë°©í•™  VLA ë¡œë´‡  íŠ¹ê°•\n",
      "ìƒì„±ì\n",
      " jin lim\n",
      "ìƒì„±  ì¼ì‹œ\n",
      "ì¹´í…Œê³ ë¦¬ Research Kit î‚RKî‚‚\n",
      "ìµœì¢…  ì—…ë°ì´íŠ¸  ì‹œê°„\n",
      "\"From Simulation to ...\n",
      "\n",
      "Qwen(qwen3:0.6b)ì˜ RAG ë‹µë³€:\n",
      "ì„ì§„ì„­ ( ì„ì‚¬  ê³¼ì • ), ê¹€ì˜ˆì°¬ ( í•™ë¶€  ì—°êµ¬ìƒ )  \n",
      "ë¬¸ì˜ : l i m s k 5 1 9 @ k o o k m i n . a c . k r\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(f\"--- [RAG ì ìš© í›„ ì§ˆë¬¸: {test_question}] ---\")\n",
    "\n",
    "# 1) ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œ ë‚´ìš©(Context) ê²€ìƒ‰\n",
    "found_context = search_pdf(test_question, chunks, chunk_embeddings)\n",
    "print(f\"ğŸ” [ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš© ì¼ë¶€]:\\n{found_context[:100]}...\\n\")\n",
    "\n",
    "# 2) RAG ì „ìš© í”„ë¡¬í”„íŠ¸ ì •ì˜\n",
    "rag_template = \"\"\"\n",
    "You are a helpful assistant. Answer the question based ONLY on the context below.\n",
    "If the answer is not in the context, say \"I don't know\".\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "rag_prompt = ChatPromptTemplate.from_template(rag_template)\n",
    "\n",
    "# 3) Chain ì‹¤í–‰ (Contextì™€ Questionì„ í•¨ê»˜ ì£¼ì…)\n",
    "rag_chain = rag_prompt | llm | StrOutputParser()\n",
    "response_after = rag_chain.invoke({\n",
    "    \"context\": found_context,   # ê²€ìƒ‰í•œ ë‚´ìš©\n",
    "    \"question\": test_question   # ì‚¬ìš©ì ì§ˆë¬¸\n",
    "})\n",
    "\n",
    "print(f\"Qwen({llm.model})ì˜ RAG ë‹µë³€:\")\n",
    "print(response_after)\n",
    "print(\"-------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mujoco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
